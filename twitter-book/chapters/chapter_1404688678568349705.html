<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>75. Moar Trash-talking Singularitarianism</title>
  <link rel="stylesheet" href="../style.css">
</head>
<body>
  <div class="page">
    <nav class="book-nav"><div class="nav-left"><a class="nav-link nav-prev" href="../chapters/chapter_1404594135261016067.html"><span class="nav-arrow">&larr;</span><span class="nav-label">74. The Rule of 5</span></a></div><div class="nav-center"><a class="nav-link nav-toc" href="../toc.html">Table of Contents</a></div><div class="nav-right"><a class="nav-link nav-next" href="../chapters/chapter_1407009538218205189.html"><span class="nav-label">76. Project managing network effects</span><span class="nav-arrow">&rarr;</span></a></div></nav>
    
    <div class="page-content">
      <div class="chapter">
        <h1 class="chapter-title">75. Moar Trash-talking Singularitarianism</h1>
        <div class="chapter-subtitle">June 15, 2021</div>
        <div class="chapter-summary"><em>In which I trash-talk singularitarianism again ‚Äî arguing that AGI is an eschatological motte-and-bailey, that clever people overestimate the cosmic importance of cleverness, and that the circularity of the smartest people worrying about superintelligence is a kind of anthropic narcissism.</em></div>
        <div class="tweet">
  <div class="tweet-text">People always say ‚Äúsmartest people‚Äù and ‚Äúartificial intelligence‚Äù in the same breath ü§î<br>
<br>
There may be a slight circularity problem in this discourse. <a href="https://twitter.com/mattyglesias/status/1404595039318183939" target="_blank" rel="noreferrer">tweet</a><sup class="endnote-ref"><a href="#endnote-1">[1]</a></sup></div>



</div>
<div class="tweet">
  <div class="tweet-text">There is no AI alignment problem.</div>



</div>
<div class="tweet">
  <div class="tweet-text">I encourage you to draw the inference that I‚Äôm not one of the smartest people you know. ü§£</div>



</div>
<div class="tweet">
  <div class="tweet-text">AGI is the eschatological leap of faith that a series of mottes will converge to a bailey. The big achievement of the AGI thought-experiment crowd (not AI practice) is to label their motte-and-bailey fallacy as a ‚Äúmoving the goalposts‚Äù lack of vision on the part of skeptics.</div>



</div>
<div class="tweet">
  <div class="tweet-text">It‚Äôs roughly like believing that building better and better airplanes will converge to a time machine or hyperspace drive. The adjective ‚Äúgeneral‚Äù does way too much work in a context where ‚Äúgenerality‚Äù is a fraught matter.<br>
<br>
I should add: I don‚Äôt believe humans are AGIs either.</div>



</div>
<div class="tweet">
  <div class="tweet-text">In fact, I don‚Äôt think ‚ÄúAGI‚Äù is a well-posed concept at all. There are approximately turing complete systems, but assuming the generality of a UTM relative to the clean notion of computability is the same thing as generality of ‚Äúintelligence‚Äù is invalid.</div>



</div>
<div class="tweet">
  <div class="tweet-text">At least the really silly foundation on IQ and psychometrics is withering away. I think the Bostrom style simulationist foundation is at least fun to think about though even sillier taken literally. But it highlights the connection to the hard problem of consciousness.</div>



</div>
<div class="tweet">
  <div class="tweet-text">I‚Äôve been following this conversation since the beginning about 15 years ago, and I feel I need to re-declare my skepticism every few years, since it‚Äôs such a powerful attractor around these parts. Like periodically letting my extended religious family know I‚Äôm not religious.</div>



</div>
<div class="tweet">
  <div class="tweet-text">It‚Äôs interesting that the AGI ideology only appeared late into the AI winter, despite associated pop-tropes (Skynet etc) being around much longer. AGI is a bit like the philosopher‚Äôs stone of AI. It has sparked interesting developments just as alchemy did chemistry.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Re: AI alignment in a more practical sense of deep learning biases and explainability, I‚Äôve seen nothing new on the ethics front that is not a straightforward extrapolation of ordinary risk management and bureaucratic biases.</div>



</div>
<div class="tweet">
  <div class="tweet-text">The tech is interesting and new, as is the math. The ethics side is important, but so far nothing I‚Äôve read there strikes me as important and new or particularly unique to AI systems. Treating it as such just drives a new obscurantism.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Previous thread about this from February. Wish I‚Äôd indexed all my threads over the past 5-6 years to see if my positions have evolved or shifted. </div>

<blockquote class="quote-box"><div class="quote-text">there are no general intelligences</div></blockquote>

</div>
<div class="tweet">
  <div class="tweet-text">But to the original QT, a new point I‚Äôm adding here is the non-trivial observation that those who most believe in AGIs also happen to be convinced they are the smartest people around (and apparently manage to convince some around them, though Matt appears to be snarking).</div>



</div>
<div class="tweet">
  <div class="tweet-text">Circular like the anthropic principle. You notice that earth is optimized to sustain human life. Your first thought is, a God created this place just for us. Then you have the more sophisticated thought that if it weren‚Äôt Goldilocks optimal we wouldn‚Äôt be around to wonder why‚Ä¶</div>



</div>
<div class="tweet">
  <div class="tweet-text">But notice that the first thought posits a specific kind of extrapolation ‚Äî an egocentric extrapolation. ‚ÄúGod‚Äù is not a random construct but an extrapolation of an egocentric self-image as ‚Äúcause‚Äù of Goldilocks zone.<br>
<br>
The second thought makes it unnecessary to posit that.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Flip it around to be teleological. In this case, a certain class of people do well in a pattern of civilization. If you assume that pattern is eternal, that class of people suggest evolution to an  alluring god-like omega point and a worry that machines will get there first.</div>



</div>
<div class="tweet">
  <div class="tweet-text">But as a skeptic, you wonder‚Ä¶ if this civilization didn‚Äôt have this pattern, these people wouldn‚Äôt be around worrying about superintelligence. Some other group would be. Top dogs always fight imaginary gods.</div>



</div>
<div class="tweet">
  <div class="tweet-text">No accident that the same crowd is also most interested in living forever. A self-perpetuation drive shapes this entire thought space.</div>



</div>
<div class="tweet">
  <div class="tweet-text">This world is your oyster, you‚Äôre winning unreasonably easily and feel special. You want it to continue. You imagine going from temporarily successful human to permanently successful superhuman. Attribution bias helps pick out variables to extrapolate and competitors to fear.</div>



</div>
<div class="tweet">
  <div class="tweet-text">The alt explanation is less flattering. You‚Äôre a specialized being adapted to a specialized situation that is transient on a cosmic scale but longer than your lifespan. But it is easy and tempting to confuse a steady local co-evolution gradient (Flynn effect anyone?) for Destiny.</div>



</div>
<div class="tweet">
  <div class="tweet-text">I‚Äôm frankly waiting for a different kind of Singularity. One comparable to chemistry forking off from alchemy because it no longer needed the psychospiritual scaffolding of transmutation to gold or elixir of life to think about chemical reactions.</div>



</div>
<div class="tweet">
  <div class="tweet-text">I‚Äôm glad this subculture inspired a few talented people to build interesting bleeding edge systems at Open AI and Deep Mind. But the alchemy is starting to obscure the chemistry now.</div>



</div>
<div class="tweet">
  <div class="tweet-text">My own recent attempt to develop a ‚Äúchemistry, not alchemy‚Äù perspective <br>
<br>
<a href="https://breakingsmart.substack.com/p/superhistory-not-superintelligence" target="_blank" rel="noreferrer">Superhistory, Not Superintelligence - by Venkatesh Rao</a></div>



</div>
<div class="tweet">
  <div class="tweet-text">As usual I‚Äôve been too wordy. This is the essential point. Clever people overestimating the importance of cleverness in the grand scheme of things. <a href="https://twitter.com/TACJ/status/1404897926577541135" target="_blank" rel="noreferrer">tweet</a><sup class="endnote-ref"><a href="#endnote-2">[2]</a></sup></div>



</div>
<div class="tweet">
  <div class="tweet-text">The many sad or unimpressive life stories of Guinness record IQ types illustrates that intelligence has diminishing returns even in our own environment. If you think you‚Äôd be 2x more successful if you were 2x smarter you might be disappointed.</div>



</div>
<div class="tweet">
  <div class="tweet-text">It‚Äôs a bit like me being good at 2x2s and worrying that somebody will discover the ultimate world-destroying 2x2. Except most strengths don‚Äôt tempt you into such conceits or narcissistic projections.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Intelligence, physical strength, adversarial cunning, and beauty are among the few that do tempt people this way. Because they are totalizing aesthetic lenses on the world. When you have one of these hammers in your hand, everything looks like a nail.</div>



</div>
<div class="chapter-endnotes">
<h2 class="endnotes-title">Notes</h2>
<ol class="endnotes-list">
<li id="endnote-1" class="endnote-item">@mattyglesias &mdash; <a href="https://twitter.com/mattyglesias/status/1404595039318183939" target="_blank" rel="noreferrer">https://twitter.com/mattyglesias/status/1404595039318183939</a></li>
<li id="endnote-2" class="endnote-item">@TACJ &mdash; <a href="https://twitter.com/TACJ/status/1404897926577541135" target="_blank" rel="noreferrer">https://twitter.com/TACJ/status/1404897926577541135</a></li>
</ol>
</div>
      </div>
    </div>
    
      <nav class="book-nav"><div class="nav-left"><a class="nav-link nav-prev" href="../chapters/chapter_1404594135261016067.html"><span class="nav-arrow">&larr;</span><span class="nav-label">74. The Rule of 5</span></a></div><div class="nav-center"><a class="nav-link nav-toc" href="../toc.html">Table of Contents</a></div><div class="nav-right"><a class="nav-link nav-next" href="../chapters/chapter_1407009538218205189.html"><span class="nav-label">76. Project managing network effects</span><span class="nav-arrow">&rarr;</span></a></div></nav>
  </div>
</body>
</html>
