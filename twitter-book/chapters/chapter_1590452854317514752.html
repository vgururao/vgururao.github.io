<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>vgr: The Twitter Years (2007–22) | 102. Larger Concentrations of Money are Dumber</title>
  <meta property="og:title" content="vgr: The Twitter Years (2007–22) | 102. Larger Concentrations of Money are Dumber">
  <meta property="og:description" content="In which I argue that larger concentrations of money are dumber — each additional order of magnitude adds a layer of abstraction vulnerable to bullshit theories — and that the most effective deployment strategy is to fragment wealth to the smallest scale the problem physics will allow.">
  <meta property="og:image" content="https://venkateshrao.com/twitter-book/assets/cover.png">
  <meta property="og:type" content="book">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="vgr: The Twitter Years (2007–22) | 102. Larger Concentrations of Money are Dumber">
  <meta name="twitter:description" content="In which I argue that larger concentrations of money are dumber — each additional order of magnitude adds a layer of abstraction vulnerable to bullshit theories — and that the most effective deployment strategy is to fragment wealth to the smallest scale the problem physics will allow.">
  <meta name="twitter:image" content="https://venkateshrao.com/twitter-book/assets/cover.png">
  <link rel="stylesheet" href="../style.css">
  <link rel="stylesheet" href="../mobile.css">
</head>
<body>
  <div class="page">
    <nav class="book-nav"><div class="nav-left"><a class="nav-link nav-prev" href="../chapters/chapter_1584250758110752768.html"><span class="nav-arrow">&larr;</span><span class="nav-label">101. On the Power of Weekly Time Commitments</span></a></div><div class="nav-center"><a class="nav-link nav-toc" href="../toc.html">Table of Contents</a></div><div class="nav-right"><span class="nav-link nav-next disabled"><span class="nav-arrow">&rarr;</span></span></div></nav>
    
    <div class="page-content">
      <div class="chapter">
        <h1 class="chapter-title">102. Larger Concentrations of Money are Dumber</h1>
        <div class="chapter-subtitle">November 9, 2022</div>
        <div class="chapter-summary"><em>In which I argue that larger concentrations of money are dumber — each additional order of magnitude adds a layer of abstraction vulnerable to bullshit theories — and that the most effective deployment strategy is to fragment wealth to the smallest scale the problem physics will allow.</em></div>
        <div class="tweet">
  <div class="tweet-text">The larger the pile of money the dumber it is<br>
<br>
If you have only $10 it is probably really smart money because you’re going to think hard and object level about spending it<br>
<br>
If you have $10B, it’s being deployed mostly in &gt; $250m chunks via org charts with 7 levels of bs theories</div>



</div>
<div class="tweet">
  <div class="tweet-text">The largest object level thing you might ever buy even as a billionaire is probably like a car. Anything bigger, you’re actually buying a theory of ownership with multiple levels of abstraction each with assumptions.</div>



</div>
<div class="tweet">
  <div class="tweet-text">For eg. buying a refurbished aircraft carrier — probably biggest “existing thing” that is ever bought — means buying training, maintenance, technology transfer, etc. Above that, retrofit/upgrade roadmaps, aircraft options, fuel futures… it looks like a “thing” but is not.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Huge state purchases like <strong>new</strong> aircraft carriers, bridges, space tech, etc. you’re buying a multi-level theory of development.<br>
<br>
This is actually the problem with EA at scale. The philosophical principle is sound at $1000, sketchy at $1m, sketchy at $100m, and relish at $1b.</div>



</div>
<div class="tweet">
  <div class="tweet-text">You cannot deploy &gt;$250m on <strong>anything</strong> without a large bureaucratic org thinking through the details at multiple levels of abstraction, each vulnerable to capture by a bullshit theory.<br>
<br>
Even if object level is uncontroversial like “feed children”</div>



</div>
<div class="tweet">
  <div class="tweet-text">I know nothing about SBF personally… what mix of stupid/unlucky/unethical/moral-hazard etc was involved in the meltdown (Matt Levine explanation of using your own token as collateral seems to be all 4). But the EA thing is the novel element here.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Dunno if he was a sincere believer in the philosophy of it was some sort of influence theater larp for him, but trying to do good at the scale of billions within 1 lifetime has all the same dumb-big-money problems as deploying it for any other reason.</div>



</div>
<div class="tweet">
  <div class="tweet-text">You need multiple models of reality as you scale. I’d say if N=$ amount you want to deploy, you should use log_10(N) theories to think about it. So $1m = 6 mental models. $1B = 9. Regardless of purpose.<br>
<br>
EA is just <strong>one</strong> mental model of philanthropy. So good by itself up to $10.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Ugh major typos above<br>
<br>
The philosophical principle is sound at $1000, sketchy at $1m, sketchy at $100m, and relish at $1b.<br>
<br>
=<br>
<br>
The philosophical principle is sound at $1000, sketchy at $1m, very sketchy at $100m, and religion at $1b.</div>



</div>
<div class="tweet">
  <div class="tweet-text">I’ve supported people in making big money decisions but have not myself ever bought anything bigger than a car. That’s borderline between object level and theorized. We’ve been shopping for a house for the first time and it feels clearly like “buying a full-stack theory of life”</div>



</div>
<div class="tweet">
  <div class="tweet-text">I’ve seen singularitarians express an astonishing sort of worry, that “obviously” the highest leverage kind of future-utility-maxxing EA-giving is to AI risk and that seems a little too easy (afaict this is why this crowd loves EA like PB&amp;J)<br>
<br>
Really? Ya think?</div>



</div>
<div class="tweet">
  <div class="tweet-text">Fun math problem of the sort they’re actually geniuses at but never seem to do. If your theory of “Spend X$ on Y” rests on 7 layers of abstraction, and you’re 90% sure your thinking at each level is sound, what are the chances you’ll reach the right conclusion?<br>
<br>
0.9^7 = 0.48.</div>



</div>
<div class="tweet">
  <div class="tweet-text">This sort of thing has long been my main critique of wealth inequality. It’s not really a critique of EA in particular, but <strong>any</strong> single theory that an org proportionate in size to log(wealth) must embody to deploy wealth.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Large wealth concentrations produce stupidity at scale, <strong>whatever</strong> the theory and purpose of deployment. The most “effective” thing you can do is fragment it to the point it’s not quite as dumb. Unless the thing itself requires concentration, like a space program.</div>



</div>
<div class="tweet">
  <div class="tweet-text">When people say they want “market-based” solutions to problems instead of “massive” state programs, the underlying intuition is not about markets so much as it’s about maximum scale of deployment an individual or closed org gets to do without orders from “above”</div>



</div>
<div class="tweet">
  <div class="tweet-text">A “market-based” solution which leads to a huge corporation spending $1b government order via internal hierarchical decision-making is actually worse than a $1b government program that’s deployed as 40 $250k grants to smaller agencies. Latter is actually more market-like.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Of course this is not always possible. Not all problems can be partitioned this way. If you want to allocate $1b to a space program, giving 40 cities $250m to start 40 space programs is dumb. The problem requires concentration. But within physics constraints, unbundle the spend.</div>



</div>
<div class="tweet">
  <div class="tweet-text">Heh sorry but ironically illustrates the point of errors creeping in with abstraction. 1b/250k is 4000 not 40. Plus I typoed it elsewhere as 250m (which would be 4) <a href="https://twitter.com/Cubecumbered/status/1590471695298875393?s=20&amp;t=ciZdGQf3kPmwFNAxlCqJAA" target="_blank" rel="noreferrer">tweet</a><sup class="endnote-ref"><a href="#endnote-1">[1]</a></sup></div>



</div>
<div class="tweet">
  <div class="tweet-text">people have made this sort of error while actually spending real money, not just shitposting…</div>



</div>
<div class="tweet">
  <div class="tweet-text">I promise if someone gives me 1b to deploy, I’ll use an excel spreadsheet to do the arithmetic properly and hire an intern to crosscheck it it for decimal point and order of magnitude errors</div>



</div>
<div class="chapter-endnotes">
<h2 class="endnotes-title">Notes</h2>
<ol class="endnotes-list">
<li id="endnote-1" class="endnote-item">@Cubecumbered &mdash; <a href="https://twitter.com/Cubecumbered/status/1590471695298875393?s=20&amp;t=ciZdGQf3kPmwFNAxlCqJAA" target="_blank" rel="noreferrer">https://twitter.com/Cubecumbered/status/1590471695298875393?s=20&amp;t=ciZdGQf3kPmwFNAxlCqJAA</a></li>
</ol>
</div>
      </div>
    </div>
    
      <nav class="book-nav"><div class="nav-left"><a class="nav-link nav-prev" href="../chapters/chapter_1584250758110752768.html"><span class="nav-arrow">&larr;</span><span class="nav-label">101. On the Power of Weekly Time Commitments</span></a></div><div class="nav-center"><a class="nav-link nav-toc" href="../toc.html">Table of Contents</a></div><div class="nav-right"><span class="nav-link nav-next disabled"><span class="nav-arrow">&rarr;</span></span></div></nav>
  </div>
</body>
</html>
